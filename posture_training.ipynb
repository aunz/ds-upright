{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "posture_training",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "aOP_oJS6Ge0V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I81ACglKW0JE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "dLNUixieNnuu",
        "colab_type": "code",
        "outputId": "1bc8031d-a31b-4a52-ee5b-1dcdf602e1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "cell_type": "code",
      "source": [
        "# get the data, a.json are \"good\" posture, b.json are \"bad\" postures\n",
        "\n",
        "# !curl -F \"file=@something.ext\" https://file.io\n",
        "!curl -o a.json https://raw.githubusercontent.com/aunz/ds-upright/master/data/a.json\n",
        "!curl -o b.json https://raw.githubusercontent.com/aunz/ds-upright/master/data/b.json\n",
        "\n",
        "!ls -lah"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13.5M  100 13.5M    0     0  49.3M      0 --:--:-- --:--:-- --:--:-- 49.1M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13.5M  100 13.5M    0     0  29.2M      0 --:--:-- --:--:-- --:--:-- 29.1M\n",
            "total 28M\n",
            "drwxr-xr-x 1 root root 4.0K Jan 28 19:45 .\n",
            "drwxr-xr-x 1 root root 4.0K Jan 28 19:44 ..\n",
            "-rw-r--r-- 1 root root  14M Jan 28 19:54 a.json\n",
            "-rw-r--r-- 1 root root  14M Jan 28 19:54 b.json\n",
            "drwxr-xr-x 1 root root 4.0K Jan  8 17:14 .config\n",
            "drwxr-xr-x 1 root root 4.0K Jan  8 17:15 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "INwjI-8xNsQb",
        "colab_type": "code",
        "outputId": "8f7485d1-34f9-4197-a98e-5d86940d1c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# func to load json, extract x, y, concatenate them, turn into np.array, normalise /480\n",
        "def tmp(f):\n",
        "    tmp = json.load(f)\n",
        "    tmp_x = np.array([[j['position']['x'] for j in i['keypoints']] for i in tmp])\n",
        "    tmp_x = tmp_x.clip(0, 480)\n",
        "    tmp_x2 = 480 - tmp_x # create mirror\n",
        "    tmp_y = np.array([[j['position']['y'] for j in i['keypoints']] for i in tmp])\n",
        "    tmp_y = tmp_y.clip(0, 270)\n",
        "    tmp_name = np.array([i['name'] for i in tmp]) # a1 001, a1 002 etc\n",
        "    tmp_name2 = np.array([i['name'] + ' m ' for i in tmp]) # mirror\n",
        "    tmp1 = np.concatenate((tmp_x, tmp_y), axis=1) / 480\n",
        "    tmp2 = np.concatenate((tmp_x2, tmp_y), axis=1) / 480\n",
        "    return np.concatenate((tmp1, tmp2)), np.concatenate((tmp_name, tmp_name2))\n",
        "    \n",
        "\n",
        "with open('a.json', 'r') as f: a, a_name = tmp(f)\n",
        "with open('b.json', 'r') as f: b, b_name = tmp(f)\n",
        "    \n",
        "# combining a, b to a dataset\n",
        "\n",
        "X = np.concatenate((a, b))\n",
        "y = np.append(np.zeros(len(a)), np.ones(len(b))) # 0: a, 1: b\n",
        "\n",
        "ab_name = np.concatenate((a_name, b_name)) \n",
        "\n",
        "print(a.shape, b.shape, X.shape, y.shape, ab_name.shape)\n",
        "\n",
        "del tmp, a, b, a_name, b_name"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20950, 34) (20914, 34) (41864, 34) (41864,) (41864,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2cFwn-oHQUJx",
        "colab_type": "code",
        "outputId": "c9ab900a-1a9b-4b13-a1ef-ef252e1be84d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# split into 60% train, 20% val, 20% test\n",
        "np.random.seed(0)\n",
        "\n",
        "tmp = np.random.permutation(len(X))\n",
        "tmp_train = tmp[:round(len(tmp) * 0.6)]\n",
        "tmp_val = tmp[round(len(tmp) * 0.6):round(len(tmp) * 0.8)]\n",
        "tmp_test = tmp[round(len(tmp) * 0.8):]\n",
        "\n",
        "X_train, y_train = X[tmp_train], y[tmp_train]\n",
        "X_val, y_val = X[tmp_val], y[tmp_val]\n",
        "X_test, y_test = X[tmp_test], y[tmp_test]\n",
        "\n",
        "ab_name_train = ab_name[tmp_train]\n",
        "ab_name_val = ab_name[tmp_val]\n",
        "ab_name_test = ab_name[tmp_test]\n",
        "\n",
        "print('Train', X_train.shape, y_train.shape, y_train.mean())\n",
        "print('Val', X_val.shape, y_val.shape, y_val.mean())\n",
        "print('Test', X_test.shape, y_test.shape, y_test.mean())\n",
        "\n",
        "del tmp, tmp_train, tmp_val, tmp_test"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (25118, 34) (25118,) 0.5019507922605303\n",
            "Val (8373, 34) (8373,) 0.4951630240057327\n",
            "Test (8373, 34) (8373,) 0.4968350650901708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iF6ybHYXWwPn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "ru8ftUt2W7yX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ]
    },
    {
      "metadata": {
        "id": "yjU-8snebsbX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rdgO0g2SVY_",
        "colab_type": "code",
        "outputId": "83eec84d-a3c2-4367-d76c-48ee28532152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='auto', max_iter=1000).fit(X_train, y_train)\n",
        "\n",
        "print(model.score(X_train, y_train))\n",
        "print(model.score(X_val, y_val))\n",
        "print(model.score(X_test, y_test))\n",
        "\n",
        "# confusion_matrix(y_val, model.predict(X_val))\n",
        "\n",
        "print(classification_report(y_val, model.predict(X_val)))\n",
        "\n",
        "print(model.intercept_, model.coef_)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9371765267935345\n",
            "0.9390899319240416\n",
            "0.9361041442732593\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.97      0.94      4227\n",
            "         1.0       0.97      0.90      0.94      4146\n",
            "\n",
            "   micro avg       0.94      0.94      0.94      8373\n",
            "   macro avg       0.94      0.94      0.94      8373\n",
            "weighted avg       0.94      0.94      0.94      8373\n",
            "\n",
            "[3.00705902] [[ 3.29048868e-01  1.28142872e+00 -9.11747294e-02  3.27218577e-01\n",
            "   8.95672260e-01 -2.53840460e+00 -1.15891015e-01 -2.73165027e-01\n",
            "  -3.54217601e-01 -1.63730908e-01  1.37846257e-01  1.08912929e+00\n",
            "   3.58490368e-01  4.53682081e-01 -1.78663676e-03 -3.16846355e-01\n",
            "  -2.33700309e-01  2.43935817e+01  2.01671057e+01  1.91913009e+01\n",
            "   2.23131176e+01  8.38295185e+00 -8.00448108e+00 -6.71054376e+00\n",
            "  -8.68249393e+00 -3.72614547e+00  2.42710776e+00  9.11797004e+00\n",
            "  -1.87704717e+01 -1.72799439e+01 -9.01745569e+00 -1.14341964e+01\n",
            "   2.58584997e+00  1.24522597e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MvB3aBLGBOlm",
        "colab_type": "code",
        "outputId": "e9cc51a2-f2b0-4b85-f04c-b6c69c1aaac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "cell_type": "code",
      "source": [
        "# can use these coef_ and intercept_ for javascript\n",
        "\n",
        "def predict(x):\n",
        "    tmp = (x * model.coef_).sum() + model.intercept_\n",
        "    tmp = 1 / (1 + np.exp(-tmp))\n",
        "    return tmp\n",
        "\n",
        "for i in range(0, 20):\n",
        "    print(ab_name_val[i], predict(X_val[i]), model.predict_proba(X_val)[i][1])\n",
        "\n",
        "model.coef_[0].shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a7 020 [0.08539954] 0.0853995412193786\n",
            "a1 540 [0.22460002] 0.22460002289268655\n",
            "b4 813 m  [0.02802288] 0.028022881694207153\n",
            "b10 816 [0.98933965] 0.989339648353709\n",
            "a6 431 m  [0.39762508] 0.3976250772342188\n",
            "b10 083 m  [0.85136408] 0.8513640820851378\n",
            "b9 0042 [0.84968416] 0.8496841608975633\n",
            "a5 836 [0.29135142] 0.2913514219928503\n",
            "b6 138 m  [0.96556088] 0.9655608760193223\n",
            "a2 360 [0.12825749] 0.12825748620843244\n",
            "a4 372 m  [0.27582101] 0.2758210104866193\n",
            "a4 041 [0.17465062] 0.17465061636159862\n",
            "b3 094 m  [0.99176588] 0.9917658777877499\n",
            "a6 246 [0.22968684] 0.22968683967131415\n",
            "b11 0041 m  [0.52913162] 0.5291316189009646\n",
            "b8 084 [0.92240829] 0.9224082900787459\n",
            "a5 377 m  [0.1957198] 0.1957198043772266\n",
            "a2 295 [0.13467896] 0.13467896059249196\n",
            "a7 075 [0.08684284] 0.08684284046133575\n",
            "b8 896 m  [0.97357451] 0.9735745137178458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "6QizLZLHcwCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ]
    },
    {
      "metadata": {
        "id": "MOX7F--Ycwpb",
        "colab_type": "code",
        "outputId": "cfaf863b-f1d2-43e4-fe14-3d99294d16be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
        "\n",
        "print(model.score(X_train, y_train))\n",
        "print(model.score(X_val, y_val))\n",
        "print(model.score(X_test, y_test))\n",
        "\n",
        "# confusion_matrix(y_val, model.predict(X_val))\n",
        "\n",
        "print(classification_report(y_val, model.predict(X_val)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9991639461740585\n",
            "0.9989251164457184\n",
            "0.9990445479517497\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      4227\n",
            "         1.0       1.00      1.00      1.00      4146\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      8373\n",
            "   macro avg       1.00      1.00      1.00      8373\n",
            "weighted avg       1.00      1.00      1.00      8373\n",
            "\n",
            "CPU times: user 5.03 s, sys: 0 ns, total: 5.03 s\n",
            "Wall time: 5.03 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P6MClo3-dmmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basic neural network"
      ]
    },
    {
      "metadata": {
        "id": "O_8DMI0Bdm1_",
        "colab_type": "code",
        "outputId": "bd327135-2f41-4a21-cb16-861b57c857a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, SeparableConv2D, MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, BatchNormalization, Flatten, Dropout, InputLayer\n",
        "from keras.optimizers import Adam, Adamax, RMSprop\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVPrGT2Rg1dz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(make_model, n = 5, optimizer = lambda: 'rmsprop', callbacks = lambda: [EarlyStopping(patience=5, verbose=1)], verbose=0):\n",
        "    # given a model, train it for n times and plot the associated metrics\n",
        "    # make_model, optimizer and callbacks should be provided as a function as each time the functions are called, brand new instances are created in the for loop below. Use this because can't use deepcopy\n",
        "\n",
        "    models = [] # to hold the model weights\n",
        "    hists = [] # contains all the history\n",
        "    \n",
        "    make_model(None).summary()\n",
        "    \n",
        "    plt.figure(figsize=(4 * (n + 2), 8)) # the figure\n",
        "\n",
        "    for i in range(n):\n",
        "        model = make_model(i)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=optimizer(), metrics=['accuracy'])\n",
        "        hist = model.fit(X_train, y_train, batch_size=256, epochs=100, validation_data=(X_val, y_val), callbacks=callbacks(), verbose=verbose)        \n",
        "        hists.append(hist)\n",
        "        \n",
        "        models.append(model) # store the model\n",
        "#         model.set_weights(initial_weights) # restore to the original weights\n",
        "\n",
        "        r = range(2, len(hist.history['acc']) + 1) # starting from epoch 2, ignore the first epoch\n",
        "        plt.subplot(2, n + 1, i + 2) # plot the loss history, starting with subplot 3\n",
        "        plt.plot(r, hist.history['loss'][1:], '.-', label='Train loss') # ignore the first epoch\n",
        "        plt.plot(r, hist.history['val_loss'][1:], '.-', label='Val loss')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.subplot(2, n + 1, i + 2 + n + 1) # plot the acc history, starting with subplot 3\n",
        "        plt.plot(r, hist.history['acc'][1:], '.-', label='Train acc')\n",
        "        plt.plot(r, hist.history['val_acc'][1:], '.-', label='Val acc')\n",
        "        plt.legend()\n",
        "\n",
        "    \n",
        "    plt.subplot(2, n + 1, 1) # plot the loss summary at the first subplot\n",
        "    metrics = ['loss'] * n + ['val_loss'] * n \n",
        "    values = np.concatenate([\n",
        "        [i.history['loss'][-1] for i in hists],\n",
        "        [i.history['val_loss'][-1] for i in hists],\n",
        "    ])\n",
        "    plt.plot(metrics, values, '.')\n",
        "    plt.ylabel('Loss')\n",
        "    values = values.reshape(2, -1)\n",
        "    print('Loss', *values)\n",
        "    print('Mean', values.mean(1), 'Std', values.std(1))\n",
        "    \n",
        "    plt.subplot(2, n + 1, n + 2) # plot the acc summary at the second subplot\n",
        "    metrics = ['acc'] * n + ['val_acc'] * n\n",
        "    values = np.concatenate([\n",
        "        [i.history['acc'][-1] for i in hists],\n",
        "        [i.history['val_acc'][-1] for i in hists]\n",
        "    ])\n",
        "    plt.plot(metrics, values, '.')\n",
        "    plt.ylabel('Accuracy')\n",
        "    values = values.reshape(2, -1)\n",
        "    print('\\nAcc', *values)\n",
        "    print('Mean', values.mean(1), 'Std', values.std(1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    return models, hists\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uhdhgk7dg-p6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# just 1 hidden layer\n",
        "\n",
        "callbacks = lambda: [\n",
        "    ReduceLROnPlateau(patience=3, verbose=1, factor=0.5, min_lr=1e-5),\n",
        "    EarlyStopping(patience=5, verbose=1)\n",
        "]\n",
        "\n",
        "_ = train(lambda x: Sequential([\n",
        "    Dense(64, input_shape=(X.shape[1],), activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "]), callbacks=callbacks, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jUJiulMkjnL0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# 3 hidden layers\n",
        "callbacks = lambda: [\n",
        "    ReduceLROnPlateau(patience=3, verbose=1, factor=0.5, min_lr=1e-5),\n",
        "    EarlyStopping(patience=5, verbose=0)\n",
        "]\n",
        "\n",
        "_ = train(lambda x: Sequential([\n",
        "    Dense(64, input_shape=(X.shape[1],), activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "]), callbacks=callbacks, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArEUl17ml-vJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolution network"
      ]
    },
    {
      "metadata": {
        "id": "yriIKCyeAVDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "32375a98-8f34-4077-f185-95c9aed08940"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "zj6oXGni_ZAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0420d663-491e-4574-e651-47d80381be74"
      },
      "cell_type": "code",
      "source": [
        "# del generate_img"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "subprocess b'12G'\n",
            "length 13107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1xJZaKP1dvpX",
        "colab_type": "code",
        "outputId": "e4404aad-5abe-45b0-d059-ae1f19b2c92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "cell_type": "code",
      "source": [
        "# convert the data to 2D\n",
        "\n",
        "batch_size = 128 # can't use 256, memory crash in colab\n",
        "\n",
        "def generate_img(features, targets, multiplier = 480):\n",
        "    while 1:\n",
        "        yield_features = []\n",
        "        yield_targets = []\n",
        "        for feature, target in zip(np.int32(features * 480).clip(0, 479), targets):\n",
        "            tmp = np.zeros((480, 270, 1)) # pic of 480 * 270\n",
        "            for x, y in zip(feature[:17], feature[17:]): tmp[x, y] = [1]\n",
        "            yield_features.append(tmp)\n",
        "            yield_targets.append(target)\n",
        "    #         print('subprocess', subprocess.check_output(['free', '-h']).split()[7])\n",
        "    #         print('getsizeof', sys.getsizeof(tmp) // 1048576, 'length', len(yield_features))\n",
        "            if (len(yield_features) == batch_size):\n",
        "                yield np.array(yield_features), np.array(yield_targets)\n",
        "                yield_features = []\n",
        "                yield_targets = []\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=3, strides=1, activation='relu', input_shape=(480, 270, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit_generator(\n",
        "    generate_img(X_train, y_train),\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=generate_img(X_val, y_val),\n",
        "    validation_steps=len(X_val) // batch_size,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 478, 268, 32)      320       \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 4099328)           0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 4099329   \n",
            "=================================================================\n",
            "Total params: 4,099,649\n",
            "Trainable params: 4,099,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "196/196 [==============================] - 110s 559ms/step - loss: 0.1519 - acc: 0.9667 - val_loss: 0.0143 - val_acc: 0.9956\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 108s 550ms/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0066 - val_acc: 0.9978\n",
            "Epoch 3/10\n",
            " 26/196 [==>...........................] - ETA: 1:18 - loss: 0.0030 - acc: 0.9991"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sUBIvZ2q5jgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5950b0b8-8972-4529-d06e-99f74f746c39"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "metadata": {
        "id": "mcM-dtS_7vBm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kJUpTvI7zw_w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train2(make_model, n = 5, optimizer = lambda: 'rmsprop', callbacks = lambda: [EarlyStopping(patience=5, verbose=1)], verbose=0):\n",
        "    # given a model, train it for n times and plot the associated metrics\n",
        "    # make_model, optimizer and callbacks should be provided as a function as each time the functions are called, brand new instances are created in the for loop below. Use this because can't use deepcopy\n",
        "\n",
        "    models = [] # to hold the model weights\n",
        "    hists = [] # contains all the history\n",
        "    \n",
        "    make_model(None).summary()\n",
        "    \n",
        "    plt.figure(figsize=(4 * (n + 2), 8)) # the figure\n",
        "\n",
        "    for i in range(n):\n",
        "        model = make_model(i)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=optimizer(), metrics=['accuracy'])\n",
        "        hist = model.fit(X_train2, y_train2, batch_size=256, epochs=100, validation_data=(X_val2, y_val2), callbacks=callbacks(), verbose=verbose)        \n",
        "        hists.append(hist)\n",
        "        \n",
        "        models.append(model) # store the model\n",
        "#         model.set_weights(initial_weights) # restore to the original weights\n",
        "\n",
        "        r = range(2, len(hist.history['acc']) + 1) # starting from epoch 2, ignore the first epoch\n",
        "        plt.subplot(2, n + 1, i + 2) # plot the loss history, starting with subplot 3\n",
        "        plt.plot(r, hist.history['loss'][1:], '.-', label='Train loss') # ignore the first epoch\n",
        "        plt.plot(r, hist.history['val_loss'][1:], '.-', label='Val loss')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.subplot(2, n + 1, i + 2 + n + 1) # plot the acc history, starting with subplot 3\n",
        "        plt.plot(r, hist.history['acc'][1:], '.-', label='Train acc')\n",
        "        plt.plot(r, hist.history['val_acc'][1:], '.-', label='Val acc')\n",
        "        plt.legend()\n",
        "\n",
        "    \n",
        "    plt.subplot(2, n + 1, 1) # plot the loss summary at the first subplot\n",
        "    metrics = ['loss'] * n + ['val_loss'] * n \n",
        "    values = np.concatenate([\n",
        "        [i.history['loss'][-1] for i in hists],\n",
        "        [i.history['val_loss'][-1] for i in hists],\n",
        "    ])\n",
        "    plt.plot(metrics, values, '.')\n",
        "    plt.ylabel('Loss')\n",
        "    values = values.reshape(2, -1)\n",
        "    print('Loss', *values)\n",
        "    print('Mean', values.mean(1), 'Std', values.std(1))\n",
        "    \n",
        "    plt.subplot(2, n + 1, n + 2) # plot the acc summary at the second subplot\n",
        "    metrics = ['acc'] * n + ['val_acc'] * n\n",
        "    values = np.concatenate([\n",
        "        [i.history['acc'][-1] for i in hists],\n",
        "        [i.history['val_acc'][-1] for i in hists]\n",
        "    ])\n",
        "    plt.plot(metrics, values, '.')\n",
        "    plt.ylabel('Accuracy')\n",
        "    values = values.reshape(2, -1)\n",
        "    print('\\nAcc', *values)\n",
        "    print('Mean', values.mean(1), 'Std', values.std(1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    return models, hists\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_DIOUph5u29M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "_ = train(lambda x: Sequential([\n",
        "    Conv2D(32, kernel_size=3, strides=1, activation='relu', input_shape=(480, 270, 1)),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid')    \n",
        "]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YEPhhoj-rgtR",
        "colab_type": "code",
        "outputId": "a6bb2367-12a0-4b70-addd-406474da7256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<480x270 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 17 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}