{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "posture_training",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "aOP_oJS6Ge0V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I81ACglKW0JE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "dLNUixieNnuu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get the data, a.json are \"good\" posture, b.json are \"bad\" postures\n",
        "\n",
        "# !curl -F \"file=@something.ext\" https://file.io\n",
        "!curl -o a.json https://raw.githubusercontent.com/aunz/ds-upright/master/data/a.json\n",
        "!curl -o b.json https://raw.githubusercontent.com/aunz/ds-upright/master/data/b.json\n",
        "\n",
        "!ls -lah\n",
        "\n",
        "__import__('IPython').display.clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "INwjI-8xNsQb",
        "colab_type": "code",
        "outputId": "21819605-44e5-45c8-b300-840c1ea61b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# func to load json, extract x, y, concatenate them, turn into np.array, normalise /480\n",
        "def tmp(f):\n",
        "    tmp = json.load(f)\n",
        "    tmp_x = np.array([[j['position']['x'] for j in i['keypoints']] for i in tmp])\n",
        "    tmp_x = tmp_x.clip(0, 480)\n",
        "    tmp_x2 = 480 - tmp_x # create mirror\n",
        "    tmp_y = np.array([[j['position']['y'] for j in i['keypoints']] for i in tmp])\n",
        "    tmp_y = tmp_y.clip(0, 270)\n",
        "    tmp_name = np.array([i['name'] for i in tmp]) # a1 001, a1 002 etc\n",
        "    tmp_name2 = np.array([i['name'] + ' m ' for i in tmp]) # mirror\n",
        "    tmp1 = np.concatenate((tmp_x, tmp_y), axis=1) / 480\n",
        "    tmp2 = np.concatenate((tmp_x2, tmp_y), axis=1) / 480\n",
        "    return np.concatenate((tmp1, tmp2)), np.concatenate((tmp_name, tmp_name2))\n",
        "    \n",
        "\n",
        "with open('a.json', 'r') as f: a, a_name = tmp(f)\n",
        "with open('b.json', 'r') as f: b, b_name = tmp(f)\n",
        "    \n",
        "# combining a, b to a dataset\n",
        "\n",
        "X = np.concatenate((a, b))\n",
        "y = np.append(np.zeros(len(a)), np.ones(len(b))) # 0: a, 1: b\n",
        "\n",
        "ab_name = np.concatenate((a_name, b_name)) \n",
        "\n",
        "print(a.shape, b.shape, X.shape, y.shape, ab_name.shape)\n",
        "\n",
        "del tmp, a, b, a_name, b_name"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20950, 34) (20914, 34) (41864, 34) (41864,) (41864,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2cFwn-oHQUJx",
        "colab_type": "code",
        "outputId": "b5891256-ed15-4c0a-b9bd-0be982766995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# split into 60% train, 20% val, 20% test\n",
        "np.random.seed(0)\n",
        "\n",
        "tmp = np.random.permutation(len(X))\n",
        "tmp_train = tmp[:round(len(tmp) * 0.6)]\n",
        "tmp_val = tmp[round(len(tmp) * 0.6):round(len(tmp) * 0.8)]\n",
        "tmp_test = tmp[round(len(tmp) * 0.8):]\n",
        "\n",
        "X_train, y_train = X[tmp_train], y[tmp_train]\n",
        "X_val, y_val = X[tmp_val], y[tmp_val]\n",
        "X_test, y_test = X[tmp_test], y[tmp_test]\n",
        "\n",
        "ab_name_train = ab_name[tmp_train]\n",
        "ab_name_val = ab_name[tmp_val]\n",
        "ab_name_test = ab_name[tmp_test]\n",
        "\n",
        "print('Train', X_train.shape, y_train.shape, y_train.mean())\n",
        "print('Val', X_val.shape, y_val.shape, y_val.mean())\n",
        "print('Test', X_test.shape, y_test.shape, y_test.mean())\n",
        "\n",
        "del tmp, tmp_train, tmp_val, tmp_test"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (25118, 34) (25118,) 0.5019507922605303\n",
            "Val (8373, 34) (8373,) 0.4951630240057327\n",
            "Test (8373, 34) (8373,) 0.4968350650901708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iF6ybHYXWwPn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "ru8ftUt2W7yX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ]
    },
    {
      "metadata": {
        "id": "yjU-8snebsbX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rdgO0g2SVY_",
        "colab_type": "code",
        "outputId": "fd45578a-2364-4e6e-9d95-59fa70fbee43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(random_state=0, solver='lbfgs', multi_class='auto', max_iter=1000).fit(X_train, y_train)\n",
        "\n",
        "print(model.score(X_train, y_train))\n",
        "print(model.score(X_val, y_val))\n",
        "print(model.score(X_test, y_test))\n",
        "\n",
        "# confusion_matrix(y_val, model.predict(X_val))\n",
        "\n",
        "print(classification_report(y_val, model.predict(X_val)))\n",
        "\n",
        "# print('Intercept', f'''model.intercept_''', '\\nCoef', model.coef_)\n",
        "\n",
        "print(f'''Intercept {model.intercept_[0]:.3f}\n",
        "Coefs: {', '.join([f\"{x:.3f}\" for x in model.coef_[0]])}''')\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9371765267935345\n",
            "0.9390899319240416\n",
            "0.9361041442732593\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.91      0.97      0.94      4227\n",
            "         1.0       0.97      0.90      0.94      4146\n",
            "\n",
            "   micro avg       0.94      0.94      0.94      8373\n",
            "   macro avg       0.94      0.94      0.94      8373\n",
            "weighted avg       0.94      0.94      0.94      8373\n",
            "\n",
            "Intercept 3.007\n",
            "Coefs: 0.329, 1.281, -0.091, 0.327, 0.896, -2.538, -0.116, -0.273, -0.354, -0.164, 0.138, 1.089, 0.358, 0.454, -0.002, -0.317, -0.234, 24.394, 20.167, 19.191, 22.313, 8.383, -8.004, -6.711, -8.682, -3.726, 2.427, 9.118, -18.770, -17.280, -9.017, -11.434, 2.586, 1.245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MvB3aBLGBOlm",
        "colab_type": "code",
        "outputId": "9e0a4871-f2fa-43a1-f650-ae0b98a25b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "cell_type": "code",
      "source": [
        "# can use these coef_ and intercept_ for javascript\n",
        "\n",
        "def predict(x):\n",
        "    tmp = (x * model.coef_).sum() + model.intercept_\n",
        "    tmp = 1 / (1 + np.exp(-tmp))\n",
        "    return tmp\n",
        "\n",
        "for i in range(0, 20):\n",
        "    print(ab_name_val[i], predict(X_val[i]), model.predict_proba(X_val)[i][1])\n",
        "\n",
        "model.coef_[0].shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a7 020 [0.08539954] 0.0853995412193786\n",
            "a1 540 [0.22460002] 0.22460002289268655\n",
            "b4 813 m  [0.02802288] 0.028022881694207153\n",
            "b10 816 [0.98933965] 0.989339648353709\n",
            "a6 431 m  [0.39762508] 0.3976250772342188\n",
            "b10 083 m  [0.85136408] 0.8513640820851378\n",
            "b9 0042 [0.84968416] 0.8496841608975633\n",
            "a5 836 [0.29135142] 0.2913514219928503\n",
            "b6 138 m  [0.96556088] 0.9655608760193223\n",
            "a2 360 [0.12825749] 0.12825748620843244\n",
            "a4 372 m  [0.27582101] 0.2758210104866193\n",
            "a4 041 [0.17465062] 0.17465061636159862\n",
            "b3 094 m  [0.99176588] 0.9917658777877499\n",
            "a6 246 [0.22968684] 0.22968683967131415\n",
            "b11 0041 m  [0.52913162] 0.5291316189009646\n",
            "b8 084 [0.92240829] 0.9224082900787459\n",
            "a5 377 m  [0.1957198] 0.1957198043772266\n",
            "a2 295 [0.13467896] 0.13467896059249196\n",
            "a7 075 [0.08684284] 0.08684284046133575\n",
            "b8 896 m  [0.97357451] 0.9735745137178458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "6QizLZLHcwCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ]
    },
    {
      "metadata": {
        "id": "MOX7F--Ycwpb",
        "colab_type": "code",
        "outputId": "2c1c18ab-f9f4-4742-8895-a02b967cbdba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
        "\n",
        "print(model.score(X_train, y_train))\n",
        "print(model.score(X_val, y_val))\n",
        "print(model.score(X_test, y_test))\n",
        "\n",
        "# confusion_matrix(y_val, model.predict(X_val))\n",
        "\n",
        "print(classification_report(y_val, model.predict(X_val)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9991639461740585\n",
            "0.9989251164457184\n",
            "0.9990445479517497\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00      4227\n",
            "         1.0       1.00      1.00      1.00      4146\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      8373\n",
            "   macro avg       1.00      1.00      1.00      8373\n",
            "weighted avg       1.00      1.00      1.00      8373\n",
            "\n",
            "CPU times: user 4.91 s, sys: 3.1 ms, total: 4.92 s\n",
            "Wall time: 4.92 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P6MClo3-dmmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basic neural network"
      ]
    },
    {
      "metadata": {
        "id": "O_8DMI0Bdm1_",
        "colab_type": "code",
        "outputId": "3083d05f-7541-427c-c600-346b606529e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, SeparableConv2D, MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, BatchNormalization, Flatten, Dropout, InputLayer\n",
        "from keras.optimizers import Adam, Adamax, RMSprop\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVPrGT2Rg1dz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(make_model, n = 5, optimizer = lambda: 'rmsprop', callbacks = lambda: [EarlyStopping(patience=5, verbose=1)], verbose=0):\n",
        "    # given a model, train it for n times and plot the associated metrics\n",
        "    # make_model, optimizer and callbacks should be provided as a function as each time the functions are called, brand new instances are created in the for loop below. Use this because can't use deepcopy\n",
        "\n",
        "    models = [] # to hold the model weights\n",
        "    hists = [] # contains all the history\n",
        "    \n",
        "    make_model(None).summary()\n",
        "    \n",
        "    plt.figure(figsize=(4 * (n + 2), 8)) # the figure\n",
        "\n",
        "    for i in range(n):\n",
        "        model = make_model(i)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=optimizer(), metrics=['accuracy'])\n",
        "        hist = model.fit(X_train, y_train, batch_size=256, epochs=100, validation_data=(X_val, y_val), callbacks=callbacks(), verbose=verbose)        \n",
        "        hists.append(hist)\n",
        "        \n",
        "        models.append(model) # store the model\n",
        "#         model.set_weights(initial_weights) # restore to the original weights\n",
        "\n",
        "        r = range(2, len(hist.history['acc']) + 1) # starting from epoch 2, ignore the first epoch\n",
        "        plt.subplot(2, n + 1, i + 2) # plot the loss history, starting with subplot 3\n",
        "        plt.plot(r, hist.history['loss'][1:], '.-', label='Train loss') # ignore the first epoch\n",
        "        plt.plot(r, hist.history['val_loss'][1:], '.-', label='Val loss')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.subplot(2, n + 1, i + 2 + n + 1) # plot the acc history, starting with subplot 3\n",
        "        plt.plot(r, hist.history['acc'][1:], '.-', label='Train acc')\n",
        "        plt.plot(r, hist.history['val_acc'][1:], '.-', label='Val acc')\n",
        "        plt.legend()\n",
        "\n",
        "    \n",
        "    plt.subplot(2, n + 1, 1) # plot the loss summary at the first subplot\n",
        "    metrics = ['loss'] * n + ['val_loss'] * n \n",
        "    values = np.concatenate([\n",
        "        [i.history['loss'][-1] for i in hists],\n",
        "        [i.history['val_loss'][-1] for i in hists],\n",
        "    ])\n",
        "    plt.plot(metrics, values, '.')\n",
        "    plt.ylabel('Loss')\n",
        "    values = values.reshape(2, -1)\n",
        "    print('Loss', *values)\n",
        "    print('Mean', values.mean(1), 'Std', values.std(1))\n",
        "    \n",
        "    plt.subplot(2, n + 1, n + 2) # plot the acc summary at the second subplot\n",
        "    metrics = ['acc'] * n + ['val_acc'] * n\n",
        "    values = np.concatenate([\n",
        "        [i.history['acc'][-1] for i in hists],\n",
        "        [i.history['val_acc'][-1] for i in hists]\n",
        "    ])\n",
        "    plt.plot(metrics, values, '.')\n",
        "    plt.ylabel('Accuracy')\n",
        "    values = values.reshape(2, -1)\n",
        "    print('\\nAcc', *values)\n",
        "    print('Mean', values.mean(1), 'Std', values.std(1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    return models, hists\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uhdhgk7dg-p6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# just 1 hidden layer\n",
        "\n",
        "callbacks = lambda: [\n",
        "    ReduceLROnPlateau(patience=3, verbose=1, factor=0.5, min_lr=1e-5),\n",
        "    EarlyStopping(patience=5, verbose=1)\n",
        "]\n",
        "\n",
        "_ = train(lambda x: Sequential([\n",
        "    Dense(64, input_shape=(X.shape[1],), activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "]), callbacks=callbacks, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jUJiulMkjnL0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# 3 hidden layers\n",
        "callbacks = lambda: [\n",
        "    ReduceLROnPlateau(patience=3, verbose=1, factor=0.5, min_lr=1e-5),\n",
        "    EarlyStopping(patience=5, verbose=0)\n",
        "]\n",
        "\n",
        "_ = train(lambda x: Sequential([\n",
        "    Dense(64, input_shape=(X.shape[1],), activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "]), callbacks=callbacks, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArEUl17ml-vJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolution network"
      ]
    },
    {
      "metadata": {
        "id": "1xJZaKP1dvpX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert the data to 2D\n",
        "\n",
        "batch_size = 128 # can't use 256, memory crash in colab\n",
        "\n",
        "def generate_img(features, targets, multiplier = 480):\n",
        "    while 1:\n",
        "        yield_features = []\n",
        "        yield_targets = []\n",
        "        for feature, target in zip(np.int32(features * 480).clip(0, 479), targets):\n",
        "            tmp = np.zeros((270, 480, 1)) # pic of 270 * 480 (H * W)\n",
        "            for x, y in zip(feature[17:], feature[:17]): tmp[x, y] = [1]\n",
        "            yield_features.append(tmp)\n",
        "            yield_targets.append(target)\n",
        "    #         print('subprocess', subprocess.check_output(['free', '-h']).split()[7])\n",
        "    #         print('getsizeof', sys.getsizeof(tmp) // 1048576, 'length', len(yield_features))\n",
        "            if (len(yield_features) == batch_size):\n",
        "                yield np.array(yield_features), np.array(yield_targets)\n",
        "                yield_features = []\n",
        "                yield_targets = []\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=3, strides=1, activation='relu', input_shape=(270, 480, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit_generator(\n",
        "    generate_img(X_train, y_train),\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=generate_img(X_val, y_val),\n",
        "    validation_steps=len(X_val) // batch_size,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ovel6SadQKNd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolution network with image augmentation"
      ]
    },
    {
      "metadata": {
        "id": "YEPhhoj-rgtR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "h = 135\n",
        "w = 240\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.05,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        ")\n",
        "\n",
        "def generate_img(features, targets):\n",
        "    while 1:\n",
        "        yield_features = []\n",
        "        yield_targets = []\n",
        "        reduced_features = np.round(features * w)\n",
        "        reduced_features = np.int32(reduced_features).clip(0, w - 1)\n",
        "        for feature, target in zip(reduced_features, targets):\n",
        "            tmp = np.zeros((h, w, 1)) # pic of 135 * 240 (H * W)\n",
        "            for x, y in zip(feature[17:], feature[:17]): tmp[x, y] = [1]\n",
        "            yield_features.append(tmp)\n",
        "            yield_targets.append(target)\n",
        "            if (len(yield_features) == batch_size):\n",
        "#                 tmp = np.array(yield_features), np.array(yield_targets)\n",
        "                yield next(datagen.flow(np.array(yield_features), np.array(yield_targets), batch_size=batch_size))\n",
        "                yield_features = []\n",
        "                yield_targets = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDKu8Hjcld--",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=3, strides=1, activation='relu', input_shape=(h, w, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit_generator(\n",
        "    generate_img(X_train, y_train),\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=generate_img(X_val, y_val),\n",
        "    validation_steps=len(X_val) // batch_size,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z2qR7qUFbHnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1358
        },
        "outputId": "90ecd0e7-348d-4545-c9a8-d9f29c7dc533"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(135, 240, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit_generator(\n",
        "    generate_img(X_train, y_train),\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    epochs=50,\n",
        "    validation_data=generate_img(X_val, y_val),\n",
        "    validation_steps=len(X_val) // batch_size,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 133, 238, 32)      320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 133, 238, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 133, 238, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 66, 119, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 64, 117, 32)       9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 64, 117, 32)       128       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 64, 117, 32)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 32, 58, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 30, 56, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 30, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 30, 56, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 15, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_22 (Flatten)         (None, 26880)             0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 64)                1720384   \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 65        \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 1,749,025\n",
            "Trainable params: 1,748,769\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "196/196 [==============================] - 112s 571ms/step - loss: 0.6357 - acc: 0.7960 - val_loss: 0.4005 - val_acc: 0.8834\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 105s 538ms/step - loss: 0.3093 - acc: 0.8746 - val_loss: 0.3352 - val_acc: 0.9016\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 106s 538ms/step - loss: 0.2724 - acc: 0.8980 - val_loss: 6.2296 - val_acc: 0.5127\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 106s 541ms/step - loss: 0.2444 - acc: 0.9113 - val_loss: 7.7203 - val_acc: 0.5052\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 106s 540ms/step - loss: 0.2340 - acc: 0.9130 - val_loss: 7.9757 - val_acc: 0.5052\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 106s 542ms/step - loss: 0.2173 - acc: 0.9226 - val_loss: 0.2802 - val_acc: 0.9185\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 106s 540ms/step - loss: 0.2097 - acc: 0.9236 - val_loss: 8.0536 - val_acc: 0.4948\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 107s 544ms/step - loss: 0.2020 - acc: 0.9304 - val_loss: 8.0536 - val_acc: 0.4948\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 107s 544ms/step - loss: 0.2003 - acc: 0.9313 - val_loss: 3.0998 - val_acc: 0.6076\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 106s 541ms/step - loss: 0.1950 - acc: 0.9332 - val_loss: 7.6918 - val_acc: 0.5055\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 106s 541ms/step - loss: 0.1887 - acc: 0.9344 - val_loss: 6.8685 - val_acc: 0.4971\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 106s 542ms/step - loss: 0.1852 - acc: 0.9374 - val_loss: 8.0536 - val_acc: 0.4948\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 106s 543ms/step - loss: 0.1807 - acc: 0.9393 - val_loss: 8.0536 - val_acc: 0.4948\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 107s 544ms/step - loss: 0.1798 - acc: 0.9381 - val_loss: 8.0536 - val_acc: 0.4948\n",
            "Epoch 15/50\n",
            "150/196 [=====================>........] - ETA: 19s - loss: 0.1762 - acc: 0.9387"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9pB9PeNwZP4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d81ac8c-afc8-4e4d-94e6-5afb264ff867"
      },
      "cell_type": "code",
      "source": [
        "model"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f8607530c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "i7cKxZrUWxyM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}